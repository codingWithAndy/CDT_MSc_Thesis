{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from itertools import combinations as combs\n",
    "from spacy.matcher import Matcher"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1  = nlp(u'An Englishman, a Scotsman and an Irishman walk into a bar. The Englishman wanted to go so they all had to leave. #Brexitjokes')\n",
    "doc2  = nlp(u'Why do we need any colour passport? We should just be able to shout, “British! Less of your nonsense!” and stroll straight through.')\n",
    "doc3  = nlp(u'Q: With Britain leaving the EU how much space was created? A: Exactly 1GB')\n",
    "doc4  = nlp(u'VOTERS: we want to give a boat a ridiculous name UK: no VOTERS: we want to break up the EU and trash the world economy UK: fine')\n",
    "doc5  = nlp(u'#BrexitJokes How did the Brexit chicken cross the road? \\\"I never said there was a road. Or a chicken\\\".')\n",
    "doc6  = nlp(u'After #brexit, when rapper 50 cent performs in GBR he\\'ll appear as 10.00 pounds. #brexitjokes')\n",
    "doc7  = nlp(u'I long for the simpler days when #Brexit was just a term for leaving brunch early.')\n",
    "doc8  = nlp(u'Say goodbye to croissants, people. Delicious croissants. We\\'re stuck with crumpets FOREVER.')\n",
    "doc9  = nlp(u'Hello, I am from Britain, you know, the one that got tricked by a bus')\n",
    "doc10 = nlp(u'How many Brexiteers does it take to change a light bulb? None, they are all walked out because they didn’t like the way the electrician did it.')\n",
    "\n",
    "docs = [\n",
    "    doc1,\n",
    "    doc2,\n",
    "    doc3,\n",
    "    doc4,\n",
    "    doc5,\n",
    "    doc6,\n",
    "    doc7,\n",
    "    doc8,\n",
    "    doc9,\n",
    "    doc10]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part of Speach Tagging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Named Entity Recognition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def show_ents(doc):\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(f'{ent.text} - {ent.label_} - {spacy.explain(ent.label_)}')\n",
    "    else:\n",
    "        print('No entites found')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "tweet_no = 1\n",
    "for doc in docs:\n",
    "    print(f'Tweet: {tweet_no}')\n",
    "    show_ents(doc)\n",
    "    print('\\n')\n",
    "    tweet_no += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tweet: 1\n",
      "Scotsman - PERSON - People, including fictional\n",
      "Irishman - NORP - Nationalities or religious or political groups\n",
      "Englishman - PERSON - People, including fictional\n",
      "\n",
      "\n",
      "Tweet: 2\n",
      "British - NORP - Nationalities or religious or political groups\n",
      "\n",
      "\n",
      "Tweet: 3\n",
      "Britain - GPE - Countries, cities, states\n",
      "EU - ORG - Companies, agencies, institutions, etc.\n",
      "\n",
      "\n",
      "Tweet: 4\n",
      "UK - GPE - Countries, cities, states\n",
      "EU - ORG - Companies, agencies, institutions, etc.\n",
      "\n",
      "\n",
      "Tweet: 5\n",
      "Brexit - PERSON - People, including fictional\n",
      "\n",
      "\n",
      "Tweet: 6\n",
      "50 cent - MONEY - Monetary values, including unit\n",
      "10.00 pounds - MONEY - Monetary values, including unit\n",
      "\n",
      "\n",
      "Tweet: 7\n",
      "the simpler days - DATE - Absolute or relative dates or periods\n",
      "Brexit - PERSON - People, including fictional\n",
      "\n",
      "\n",
      "Tweet: 8\n",
      "FOREVER - WORK_OF_ART - Titles of books, songs, etc.\n",
      "\n",
      "\n",
      "Tweet: 9\n",
      "Britain - GPE - Countries, cities, states\n",
      "\n",
      "\n",
      "Tweet: 10\n",
      "Brexiteers - WORK_OF_ART - Titles of books, songs, etc.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sentiment Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tweet Similarity Scoring"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "spans = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for j,doc in enumerate(docs):\n",
    "    named_entity_span = [doc[i].text for i in range(len(doc)) if doc[i].ent_type != 0]\n",
    "    print(named_entity_span)\n",
    "    named_entity_span = ' '.join(named_entity_span)\n",
    "    named_entity_span = nlp(named_entity_span)\n",
    "    spans.update({j:named_entity_span})"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Scotsman', 'Irishman', 'Englishman']\n",
      "['British']\n",
      "['Britain', 'EU']\n",
      "['UK', 'EU']\n",
      "['Brexit']\n",
      "['50', 'cent', '10.000', 'pounds']\n",
      "['the', 'simpler', 'days', 'Brexit']\n",
      "['FOREVER']\n",
      "['Britain']\n",
      "['Brexiteers']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "tweet_id = [i for i in range(1,11)]\n",
    "id_combs = list(combs(tweet_id, 2))\n",
    "\n",
    "for each_pair in id_combs:\n",
    "    similarity = spans[each_pair[0]-1].similarity(spans[each_pair[1]-1])\n",
    "    #print(f'doc{each_pair[0]} is similar to doc{each_pair[1]} by: {similarity}') #Un-comment if you want to see individual scores printed.\n",
    "    results = {\n",
    "        'tweet1': int(each_pair[0]),\n",
    "        'tweet2': int(each_pair[1]),\n",
    "        'similarity': similarity\n",
    "    }\n",
    "    \n",
    "    df = df.append(results, ignore_index=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-15-5629a5f94371>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = spans[each_pair[0]-1].similarity(spans[each_pair[1]-1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Chaning Data Types\n",
    "df['tweet1'] = df['tweet1'].astype(int)\n",
    "df['tweet2'] = df['tweet2'].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Saving to/loading from CSV\n",
    "#df = pd.read_csv('similarity_scores.csv') #Uncomment to load.\n",
    "#df.to_csv('similarity_scores.csv') #Uncomment to resave."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_ordered = df.sort_values(by=['similarity'], ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Display the Top 10 Simialr Combinations \n",
    "df_ordered.head(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    similarity  tweet1  tweet2\n",
       "17    0.857896       3       4\n",
       "1     0.788178       1       3\n",
       "33    0.771924       5       9\n",
       "2     0.720223       1       4\n",
       "18    0.688950       3       5\n",
       "22    0.646520       3       9\n",
       "24    0.598866       4       5\n",
       "16    0.549264       2      10\n",
       "7     0.510660       1       9\n",
       "3     0.510251       1       5"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>tweet1</th>\n",
       "      <th>tweet2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.857896</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.788178</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.771924</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.720223</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.688950</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.646520</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.598866</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.549264</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.510660</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510251</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Display the Bottom 10 Simialr Combinations \n",
    "df_ordered.tail(10)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    similarity  tweet1  tweet2\n",
       "6     0.198919       1       8\n",
       "30    0.196068       5       6\n",
       "39    0.185533       7       8\n",
       "19    0.132649       3       6\n",
       "4     0.128754       1       6\n",
       "41    0.124216       7      10\n",
       "25    0.090598       4       6\n",
       "38    0.069899       6      10\n",
       "36    0.055461       6       8\n",
       "12    0.001826       2       6"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>tweet1</th>\n",
       "      <th>tweet2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.198919</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.196068</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.185533</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.132649</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.128754</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.124216</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.090598</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.069899</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.055461</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.001826</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utterence Pattern Matching"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def dep_pattern(doc):\n",
    "    for i in range(len(doc)-1):\n",
    "        if doc[i].dep_ == 'nsubj' and doc[i+1].dep_ == 'aux' and doc[i+2].dep_ == 'ROOT':\n",
    "            for tok in doc[i+2].children:\n",
    "                if tok.dep_ == 'dobj':\n",
    "                    return True\n",
    "    else:\n",
    "        return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "for i in docs:\n",
    "    if dep_pattern(i):\n",
    "        print('Found')\n",
    "    else:\n",
    "        print('Not Found')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Not Found\n",
      "Not Found\n",
      "Not Found\n",
      "Not Found\n",
      "Not Found\n",
      "Found\n",
      "Not Found\n",
      "Not Found\n",
      "Not Found\n",
      "Not Found\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding Word Sequence Patterns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'DEP':\"nsubj\"}, {\"DEP\":\"aux\"}, {\"DEP\":\"ROOT\"}]\n",
    "matcher.add(\"NsubjAuxRoot\", [pattern])\n",
    "\n",
    "tweet_no = 1\n",
    "\n",
    "for doc in docs:\n",
    "    matches = matcher(doc)\n",
    "    print(f'Tweet: {tweet_no}')\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        print(f\"Span: {span.text}\")\n",
    "        print(f\"The position in the doc are: {start} - {end}\\n\")\n",
    "    else:\n",
    "        print(\"None found.\\n\")\n",
    "    tweet_no += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tweet: 1\n",
      "None found.\n",
      "\n",
      "Tweet: 2\n",
      "None found.\n",
      "\n",
      "Tweet: 3\n",
      "None found.\n",
      "\n",
      "Tweet: 4\n",
      "None found.\n",
      "\n",
      "Tweet: 5\n",
      "None found.\n",
      "\n",
      "Tweet: 6\n",
      "Span: he'll appear\n",
      "The position in the doc are: 11 - 14\n",
      "\n",
      "None found.\n",
      "\n",
      "Tweet: 7\n",
      "None found.\n",
      "\n",
      "Tweet: 8\n",
      "None found.\n",
      "\n",
      "Tweet: 9\n",
      "None found.\n",
      "\n",
      "Tweet: 10\n",
      "None found.\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4853882b697e6b0431e01bed671aeb3a0e9ddaa05d67de9c41ea02ac68e863a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}