% !TEX root = ../thesis.tex
\chapter{Introduction}
	\label{chap:intro}
	We have set out to create a tool that can simulate a small scale comparative judgement experiment on what users think about tweets getting compared against each other. This experiment is in light of our stakeholder getting commissioned by the Welsh government to implement a comparative judgement system nationally for all schools in Wales. Comparative judgement is a technique that has been around for almost 100 years \cite{thurstone1927law}. However, while the process can improve results and reduce cognitive loads for teachers and markers, especially at the scale that the stakeholder's implementation will have to work at, it can still require many combinations to be marked and compared. For this experiment, we decided to use tweets based on Brexit to see wat ones people found funnier.
	
	Therefore, we have created a tool that allows users to see a sub-sample of the combinations. Once the users have viewed the varieties, an overall ranking of the results will get created. Two methods got implemented, a more traditional comparative judgement method and an Elo style ranking.
	
	We then aimed to use NLP techniques to extract any insights we could find within the tweets. We intended to extract information on the tweets to see if we could find patterns that would give us insights into what might have impacted the tweets final scores.
	
	The study got broken up into two parts. Part one was a web app to gather user's views on the tweets, and the second part was exploring NLP techniques within a Jupyter Notebook. With our aim to see if we can generate any feedback about the tweet.


	\section{Motivations}
		\label{sec:intro_motivation} 
	For the prior eight years, we have had involvement in some form of an educational environment. Seven of these years involve being a teacher within secondary and sixth form schools. While the focus of teaching is perceived to create lessons for students to learn and grow, we found more and more as the years went on that this wasn't the case. The focus was actually on providing reports about the students, which required data about the students from formal assessments. While having assessments to gauge the level that a student is at is an essential part of education. However, creating, marking, analysing and providing feedback for 30 students or more per class is a time-consuming task. Therefore, this assessment practice takes away the educators' time to do what is essential, creating meaningful lessons tailored for the students.
	
	Therefore, our motivation is to create a tool for educators that will empower them to allow technology to do what it is good at and focus on what they are good at, while aiding teachers with their decision making and allowing them to create and delivering lessons. To shape future generations views.


	%\section{Objective}
	%	\label{sec:intro_objective} 


	\section{Existing Liturature}
	Within education, teaching and learning have provided assessments to rank students' attainment since 1988s \cite{education1988}. Due to the students getting assessed, this allowed the teachers to give feedback to learners, allowing them to improve, especially with the introduction of Key Stage (KS) 1, 2 and 3, national curriculums and tests \cite{hutchison1994reliable, dillon2011becoming}. This newfound focus brought about new areas of tools and techniques for teachers to use. These new tools are called Assessment of Learning (AoL) and Assessment for Learning (AfL) \cite{wellington2007secondary, dillon2011becoming, black1998inside}. However, marking and providing feedback can be quite a time-consuming labours task, adding to workload and teacher stress. Especially when school marking policies are in place, and a certain level of marking needs to get done within a specific time frame. Additionally, teachers might implement bias towards students results by basing performance results on how they have done all year, rather than in the face value of the actual assessment.
	
	However, a newfound focus on an approach called Adaptive Comparative Judgement (ACJ) has started to make some traction \cite{pollitt2012method}. ACJ is an altered approach to Louis Leon Thurstone's the Law of Comparative Judgement (LCJ) \cite{thurstone1927psychophysical}. The LCJ and ACJ both provide a combination of examples and asks the user to judge which one out of the two is better. However, ACJ is the method proposed more within education based on its ability to be 'adaptive' in comparing the students work. Instead of every combination getting seen, it can change to make pieces of work that are considered similar get compared more to find out which one is better. ACJ  claims to be highly accurate, reduce teachers' workload, and provide meaningful feedback to the students \cite{rm_website}. However, a study found out that the method used within ACJ (rounds) makes the results biased, especially the more rounds there are. This bias demonstrates that being 'adaptive' has no more effectiveness over just having random pairings at all \cite{bramley2015investigating}. Some studies also found that the ACJ can take longer than stardard marking using a rubric \cite{mcmahon2015comparative, steedle2016evaluating}.
	
	Additionally, the feedback it provides is very minimal. Therefore, students do not receive any form of personalised feedback. Instead, they have to rely on their understanding of the task and then extract what they think is important based on their peers' work. As a result, likely to be excluding low ability students from gaining meaningful insights on how to improve.
	
	Therefore, additional avenues get explored. These are regarding other ranking systems and Natural Language Processing (NLP) to provide feedback to the users. The alternative rankings systems, Elo and Glicko \cite{elo1978rating, glickman1995glicko, glickman2012example}, are both well-used. Both ranking systems got created to score competitive chess players, with Elo was the first proposed system over the original and then the Glicko system. Both systems look into creating a score that updates on an outcome's results, with the score getting based on the likelihood probability that one entity will win over the other. The main difference between them is the stages required to calculate the score. In comparison, the Glicko system presents improvements over the perceived pitfalls of player manipulation in the Elo system like player rating protection, selective pairing and rating inflation and deflation.
	
	\section{New Insights}
	While the comparative judgement technique has many great features, we believe that the concept can still improve. We believe this is especially the case when the comparative judgment system gets expected to get done at a national scale. We believe this because the traditional method would expect all unique pairings to get compared. Additionally, the adaptive comparative judgement that most other systems have adopted still requires time and effort even when the number of individual student work is only around thirty. Therefore, it would be tough to do when needed to get scaled up to a national level. That is why we believe a different ranking system, like an Elo system, could replace the adaptive comparative judgement process and have a more crowed sourced approach. Therefore, reducing cognitive load and the time cost it would take for people to partake. 
	
	Furthermore, the current implementations do not provide any meaningful feedback to the students or educators about what makes a piece of work better than the other. Therefore, we think we can look into NLP techniques that can provide some form of feedback. To see if this can become something more meaningful and give some insights. Marking and giving feedback is a crucial role for all educators and the students receiving the feedback.


	\section{Contributions} 
		\label{sec:intro_contribs} 

		The main contributions of this work can be seen as follows:

		\begin{description}	

			\item[\(\bullet\) A web applicaiton to conduct the comparative judgement]\hfill

			We created a web application and hosted it to crowdsource users views on ten tweets based on Brexit. The app provided at random five unique pair comparisons while updating the CJ score and Elo score. 

			\item[\(\bullet\) A comparion of two different ranking systems]\hfill

			Metrics are being stored and calculated based on the two ranking systems, a CJ style and an Elo ranking system. Therefore, the results provide us with a way to compare the effectiveness of the two ranking systems. As a result, they are allowing us to see which one works better in our required situation.

			\item[\(\bullet\) An exploration into NLP techniques to provide feedback to the user]\hfill

		We created a Jupyter notebook exploring NLP information extraction techniques to provide feedback to the user from information extracted from the ten tweets.

		\end{description}
	
	\section{Results Overview}
		We found that the comparative judgement (CJ) and the Elo scores were positively correlated. Therefore, the Elo score would be an adequate replacement and possibly a better alternative ranking system to use. The Elo system showed more robustness than the CJ system, especially when the CJ system provided tweets that ended up having the same score. The final order ended up getting based on which one came first within the list if two tweets had the same CJ score. However, the Elo score didn't suffer from the same problem. It also allowed and enabled a ranking to be generated, and there was no score the same.
		
		Regarding the NLP information extraction, this ended up being a mixed bag. While it provided good building blocks to build upon, it offered some insights into the tweets to provide feedback. However, the process did not offer anything significant to be used in a more formal setting. For example, within a school and giving feedback to students.
	
	\section{Overview}  
	\label{sec:intro_overview} 
	 We will first look into the background, explaining the need education has for marking, allowing educators to rank students' work, and providing feedback to students to enable them to reflect and improve. We will then look into what comparative judgement is and its different iterations. Additionally, we look into different ranking systems, with both coming from the chess world but get currently implemented in all other scenarios, like e-Sports. We then look into what Natural Language Processing (NLP) is and some techniques to help achieve what we aim to achieve within our implementation. Then finally for this section will look at other applications that aim to implement comparative judgment within them. We will then look at our methodology, explaining the tools and design approaches we decided to use. We then look at the results we found and have a discussion around these. We then finish with a conclusion and suggested further work for this project.