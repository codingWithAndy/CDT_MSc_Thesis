% !TEX root = ../thesis.tex
\chapter{Results and Discussion}
\label{chap:results}

We will first look at the web application results based on the user's feedback, and then we will look into the insights and potential feedback the NLP process could provide the user. We then also look to review the overall process. 

We will compare the web application's results against the comparative judgment, Elo ranking, and the score we created for the tweets on Twitter. With the insights of the NLP for feedback to the user, we will look at what insights got made. Additionally, we will look at if any of the knowledge extracted generated provides any meaningful feedback to the user.



\section{Tweet Ranking Results} 
\label{sec:reaults_ranking}

	\begin{figure}[h]
		\centering
		\includegraphics[width=7cm]{combination_heat_map.png}
		\caption{The web applicaitons generated results compared agaist each other.}
		\label{fig:combinations}
		
	\end{figure}
	
	Forty different users take part in the comparison judgement within the web app. Through looking at fig: \ref{fig:combinations} we can see that all combinations got displayed to the users taking part in the comparisons. We can see that tweet one and tweet five appeared the most, while the combination appearing the lowest was tweet six and seven, with one comparison getting presented to the users.
	
	
	
	When we look at winners and losers of the comparisons( see fig: \ref{fig:combination_wins}), we can see that the tweet that won the most between a specific combination was tweet four and two, with tweet four winning six times and tweet two winning only once. Additionally, when we look at the combination that appeared the most, one and five, one came out on top five times, compared to five winning between the two once.
	\begin{figure}[h]
		\centering
		\includegraphics[width=7cm]{combination_win_heat_map.png}
		\caption{A heat map of the amount of times a tweet win or lost}
		\label{fig:combination_wins}
		
	\end{figure}
	
	When we look at the winner heat map (see fig: \ref{fig:combination_wins}), we can see that two, five, six, seven and ten had moments where they didn't win a head-to-head with another tweet. Two, six, seven and ten didn't win against at least two different tweets, while the others were only against one tweet they failed to win.
	
	
	While looking at fig \ref{fig:web_app_results}, we can see that the Elo and comparative judgement ranking generated very similar results. However, as we can see, the tweets coming in 6th, 7th and 8th a slight variation in the results.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{web_app_results.png}
		\caption{The web applicaitons generated results compared agaist each other.}
		\label{fig:web_app_results}
		
	\end{figure}

	While we look at the T-rating ranking compared to the Elo ranking (see fig: \ref{fig:twitter_results_comparison}), we can see that the results ranking is very different. The tweet that came first in the T-rankings came fourth in the Elo ranking. At the same time, the tweet that came first in the Elo ranking came eighth in the T-ranking.  Tweets that done worse in the Elo ranking compared to T-ranking had an average difference in the ranked placing of 5 places, while the tweets that had a better Elo ranking compared to the T-ranking ranked an average of 4 places lower. Therefore, 4 of the top 5 tweets in the T-ranking were actually in the bottom five of the Elo ranking. Only tweet ID 4 done one place better with the Elo ranking than it did in the T-ranking. However, two of the top three tweets in the T-ranking were in the bottom three of the Elo ranking and vice versa.

	\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{twitter_results_comparison.png}
		\caption{The Twitter tweet score ranking comparison against Elo ranking.}
		\label{fig:twitter_results_comparison}
		
	\end{figure}

	Within the forty participants, twenty-two of them left a justification for why they select one tweet over the other. However, the participants' responses were varied in the amount of provided feedback. Some proved a justification for all five combinations. On the other hand, some only left them for a few and not all. The users gave a total of sixty-three explanations to their decisions on which tweet they had chosen.
	
	One user stated, "I just think it is a clever way to put our departure from EU, plus it did make me giggle." The comment was in regards to tweet three beating tweet eight. Tweet three did provide several justifications, a lot of them to do around its tech theme on Brexit. Some of the rationales are "Comp sci wordplay", "everyone loves a tech joke", "Because it's the nerdier option", the "First tweet just lol", and "Actually laughed out loud".
	
	Another tweet, tweet ten beating tweet 8, had the justification for winning as 'because of the wordplay'. So we can see that several tweets had some form of explanation around the lines of good wordplay. Therefore, creating user feedback has not made an excellent source of information to help build feedback. However, it has given some context to why they had made their decisions.


\section{NLP Feedback and Insights}
\label{sec:reaults_NLP}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{top_10_sim_NER.png}
		\caption{.}
		\label{fig:top_10_sim_NER}
		
	\end{figure}

		\begin{figure}[h]
		\centering
		\includegraphics[width=15cm]{top_10_sim_doc.png}
		\caption{.}
		\label{fig:top_10_sim_doc}
		
	\end{figure}

	\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{information_extract.png}
		\caption{.}
		\label{fig:information_extract}
		
	\end{figure}

\section{Overall Results}
\label{sec:reaults_NLP}

